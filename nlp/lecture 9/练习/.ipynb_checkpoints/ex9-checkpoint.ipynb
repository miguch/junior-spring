{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_path + 'labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test_data = pd.read_csv(data_path + 'testData.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def sentenceToWords(sentence):\n",
    "    sentence = BeautifulSoup(sentence).get_text()\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    words = sentence.lower().split()\n",
    "    return words\n",
    "\n",
    "train_data['words'] = [' '.join(sentenceToWords(sentence)) for sentence in train_data['review']]\n",
    "test_data['words'] = [' '.join(sentenceToWords(sentence)) for sentence in test_data['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)\n",
    "train_features = vectorizer.fit_transform(train_data['words'].values)\n",
    "test_features = vectorizer.transform(test_data['words'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: \n",
      "fit_time: [8.46212125 7.00664186 7.18373871 6.92363095 7.52549791]\n",
      "Average fit_time: 7.420326\n",
      "score_time: [0.48268986 0.35405421 0.35841203 0.35830808 0.35904217]\n",
      "Average score_time: 0.382501\n",
      "test_accuracy: [0.8372 0.8326 0.841  0.8434 0.8514]\n",
      "Average test_accuracy: 0.841120\n",
      "train_accuracy: [1. 1. 1. 1. 1.]\n",
      "Average train_accuracy: 1.000000\n",
      "test_f1: [0.8372 0.8326 0.841  0.8434 0.8514]\n",
      "Average test_f1: 0.841120\n",
      "train_f1: [1. 1. 1. 1. 1.]\n",
      "Average train_f1: 1.000000\n",
      "test_roc_auc: [0.91861128 0.91471744 0.91926568 0.91591616 0.92468208]\n",
      "Average test_roc_auc: 0.918639\n",
      "train_roc_auc: [1. 1. 1. 1. 1.]\n",
      "Average train_roc_auc: 1.000000\n",
      "\n",
      "\n",
      "SGD:\n",
      "fit_time: [1.07016206 1.03188992 1.03655696 0.93682003 1.07132268]\n",
      "Average fit_time: 1.029350\n",
      "score_time: [0.01092505 0.01448131 0.01440215 0.01158786 0.00974488]\n",
      "Average score_time: 0.012228\n",
      "test_accuracy: [0.8524 0.8652 0.8648 0.8624 0.8426]\n",
      "Average test_accuracy: 0.857480\n",
      "train_accuracy: [0.9557  0.94155 0.944   0.96125 0.92035]\n",
      "Average train_accuracy: 0.944570\n",
      "test_f1: [0.8524 0.8652 0.8648 0.8624 0.8426]\n",
      "Average test_f1: 0.857480\n",
      "train_f1: [0.9557  0.94155 0.944   0.96125 0.92035]\n",
      "Average train_f1: 0.944570\n",
      "test_roc_auc: [0.92936816 0.93346    0.92929264 0.93227072 0.93099264]\n",
      "Average test_roc_auc: 0.931077\n",
      "train_roc_auc: [0.99348921 0.99148581 0.99168394 0.99339795 0.99119756]\n",
      "Average train_roc_auc: 0.992251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.model_selection\n",
    "RF_params = {\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "SGD_params = {\n",
    "    'max_iter': 10000,\n",
    "    'tol': 1e-4,\n",
    "    'n_jobs': -1,\n",
    "    'loss': 'log'\n",
    "}\n",
    "def cv(x, y, model, params={}, splits=5):\n",
    "    clf = model(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }, return_train_score=True)\n",
    "    return cv_score\n",
    "\n",
    "def run_cross_validation(x, y, model, params):\n",
    "    cv_result = cv(x, y, model, params=params, splits=5)\n",
    "    for scorer, score in cv_result.items():\n",
    "        print('%s: %s' % (scorer, score))\n",
    "        print('Average %s: %f' % (scorer, score.mean()))\n",
    "        \n",
    "print('Random Forest: ')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, RandomForestClassifier, RF_params)\n",
    "print('\\n\\nSGD:')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, SGDClassifier, SGD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(**SGD_params)\n",
    "model.fit(train_features, train_data['sentiment'].values)\n",
    "result = model.predict_proba(test_features)[:, 1]\n",
    "test_data['sentiment'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle ROC-AUC score: 0.92601\n",
    "test_data[['id', 'sentiment']].to_csv('res/BOW.csv', index=False, quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def paragraphToSentences(paragraph):\n",
    "    sentences = tokenizer.tokenize(paragraph.strip())\n",
    "    return [sentenceToWords(sentence) for sentence in sentences if sentence]\n",
    "\n",
    "allSentences = []\n",
    "print('Processing Training Set')\n",
    "for review in train_data['review']:\n",
    "    allSentences.extend(paragraphToSentences(review))\n",
    "print('Processing Test Set')\n",
    "for review in test_data['review']:\n",
    "    allSentences.extend(paragraphToSentences(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "word2vec_model = word2vec.Word2Vec(allSentences, workers=12, \n",
    "            size=300, min_count = 40, \n",
    "            window = 10, sample = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6721714735031128),\n",
       " ('lady', 0.574566662311554),\n",
       " ('soldier', 0.5583536028862),\n",
       " ('priest', 0.5465677976608276),\n",
       " ('guy', 0.543327271938324),\n",
       " ('businessman', 0.5413163304328918),\n",
       " ('lad', 0.5302108526229858),\n",
       " ('boy', 0.5284541845321655),\n",
       " ('doctor', 0.519687831401825),\n",
       " ('men', 0.5051175355911255)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12857, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words):\n",
    "    featureVec = np.zeros(shape=(word2vec_model.wv.vectors.shape[1],), dtype=\"float32\")\n",
    "    count = 0\n",
    "    word_set = set(word2vec_model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in word_set:\n",
    "            count += 1\n",
    "            featureVec += word2vec_model.wv[word]\n",
    "    return featureVec / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVec(sentences):\n",
    "    res = np.empty(shape=(len(sentences), word2vec_model.wv.vectors.shape[1]), dtype=\"float32\")\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        if index % 5000 == 0:\n",
    "            print(index)\n",
    "        res[index] = makeFeatureVec(sentence.split())\n",
    "        index += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_features = getAvgFeatureVec(train_data['words'])\n",
    "test_features = getAvgFeatureVec(test_data['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD:\n",
      "fit_time: [0.66455102 1.311234   0.85900378 1.12196302 0.83773685]\n",
      "Average fit_time: 0.958898\n",
      "score_time: [0.02380586 0.01853895 0.018049   0.01880908 0.01573992]\n",
      "Average score_time: 0.018989\n",
      "test_accuracy: [0.8688 0.8578 0.853  0.8636 0.8568]\n",
      "Average test_accuracy: 0.860000\n",
      "train_accuracy: [0.8648  0.86805 0.8608  0.8677  0.86745]\n",
      "Average train_accuracy: 0.865760\n",
      "test_f1: [0.8688 0.8578 0.853  0.8636 0.8568]\n",
      "Average test_f1: 0.860000\n",
      "train_f1: [0.8648  0.86805 0.8608  0.8677  0.86745]\n",
      "Average train_f1: 0.865760\n",
      "test_roc_auc: [0.938292   0.93376928 0.9333984  0.93520064 0.93183568]\n",
      "Average test_roc_auc: 0.934499\n",
      "train_roc_auc: [0.93747448 0.93898884 0.93860895 0.93819742 0.93918329]\n",
      "Average train_roc_auc: 0.938491\n"
     ]
    }
   ],
   "source": [
    "print('SGD:')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, SGDClassifier, SGD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(**SGD_params)\n",
    "model.fit(train_features, train_data['sentiment'].values)\n",
    "result = model.predict_proba(test_features)[:, 1]\n",
    "test_data['sentiment'] = result\n",
    "# Kaggle ROC-AUC score: 0.93751\n",
    "test_data[['id', 'sentiment']].to_csv('res/Word2Vec.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
