{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_path + 'labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test_data = pd.read_csv(data_path + 'testData.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def sentenceToWords(sentence):\n",
    "    sentence = BeautifulSoup(sentence).get_text()\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    words = sentence.lower().split()\n",
    "    return words\n",
    "\n",
    "train_data['words'] = [' '.join(sentenceToWords(sentence)) for sentence in train_data['review']]\n",
    "test_data['words'] = [' '.join(sentenceToWords(sentence)) for sentence in test_data['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)\n",
    "train_features = vectorizer.fit_transform(train_data['words'].values)\n",
    "test_features = vectorizer.transform(test_data['words'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: \n",
      "fit_time: [7.64325404 7.30175304 7.48292899 7.76622891 7.38657403]\n",
      "Average fit_time: 7.516148\n",
      "score_time: [0.36505389 0.36059999 0.357687   0.35433722 0.35551095]\n",
      "Average score_time: 0.358638\n",
      "test_accuracy: [0.8474 0.8424 0.8378 0.8368 0.8406]\n",
      "Average test_accuracy: 0.841000\n",
      "train_accuracy: [1. 1. 1. 1. 1.]\n",
      "Average train_accuracy: 1.000000\n",
      "test_f1: [0.8474 0.8424 0.8378 0.8368 0.8406]\n",
      "Average test_f1: 0.841000\n",
      "train_f1: [1. 1. 1. 1. 1.]\n",
      "Average train_f1: 1.000000\n",
      "test_roc_auc: [0.923602   0.91951944 0.91685456 0.9169128  0.91790816]\n",
      "Average test_roc_auc: 0.918959\n",
      "train_roc_auc: [1. 1. 1. 1. 1.]\n",
      "Average train_roc_auc: 1.000000\n",
      "\n",
      "\n",
      "SGD:\n",
      "fit_time: [1.00066376 1.24992776 1.27040219 1.001261   1.08128214]\n",
      "Average fit_time: 1.120707\n",
      "score_time: [0.01230001 0.00980926 0.0099709  0.00990391 0.01044583]\n",
      "Average score_time: 0.010486\n",
      "test_accuracy: [0.8612 0.8624 0.8594 0.8546 0.8638]\n",
      "Average test_accuracy: 0.860280\n",
      "train_accuracy: [0.9636  0.9528  0.94555 0.95185 0.944  ]\n",
      "Average train_accuracy: 0.951560\n",
      "test_f1: [0.8612 0.8624 0.8594 0.8546 0.8638]\n",
      "Average test_f1: 0.860280\n",
      "train_f1: [0.9636  0.9528  0.94555 0.95185 0.944  ]\n",
      "Average train_f1: 0.951560\n",
      "test_roc_auc: [0.93072448 0.93096016 0.93102832 0.92881056 0.9326792 ]\n",
      "Average test_roc_auc: 0.930841\n",
      "train_roc_auc: [0.99408917 0.99251982 0.99239486 0.99264582 0.9918027 ]\n",
      "Average train_roc_auc: 0.992690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.model_selection\n",
    "RF_params = {\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "SGD_params = {\n",
    "    'max_iter': 10000,\n",
    "    'tol': 1e-4,\n",
    "    'n_jobs': -1,\n",
    "    'loss': 'log'\n",
    "}\n",
    "def cv(x, y, model, params={}, splits=5):\n",
    "    clf = model(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }, return_train_score=True)\n",
    "    return cv_score\n",
    "\n",
    "def run_cross_validation(x, y, model, params):\n",
    "    cv_result = cv(x, y, model, params=params, splits=5)\n",
    "    for scorer, score in cv_result.items():\n",
    "        print('%s: %s' % (scorer, score))\n",
    "        print('Average %s: %f' % (scorer, score.mean()))\n",
    "        \n",
    "print('Random Forest: ')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, RandomForestClassifier, RF_params)\n",
    "print('\\n\\nSGD:')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, SGDClassifier, SGD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(**SGD_params)\n",
    "model.fit(train_features, train_data['sentiment'].values)\n",
    "result = model.predict_proba(test_features)[:, 1]\n",
    "test_data['sentiment'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle ROC-AUC score: 0.92870\n",
    "test_data[['id', 'sentiment']].to_csv('res/BOW.csv', index=False, quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def paragraphToSentences(paragraph):\n",
    "    sentences = tokenizer.tokenize(paragraph.strip())\n",
    "    return [sentenceToWords(sentence) for sentence in sentences if sentence]\n",
    "\n",
    "allSentences = []\n",
    "print('Processing Training Set')\n",
    "for review in train_data['review']:\n",
    "    allSentences.extend(paragraphToSentences(review))\n",
    "print('Processing Test Set')\n",
    "for review in test_data['review']:\n",
    "    allSentences.extend(paragraphToSentences(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "word2vec_model = word2vec.Word2Vec(allSentences, workers=12, \n",
    "            size=300, min_count = 40, \n",
    "            window = 10, sample = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6817048788070679),\n",
       " ('lady', 0.5643702149391174),\n",
       " ('soldier', 0.549045205116272),\n",
       " ('businessman', 0.5414022207260132),\n",
       " ('boy', 0.5407766699790955),\n",
       " ('guy', 0.5258374214172363),\n",
       " ('priest', 0.5237928032875061),\n",
       " ('lad', 0.5100698471069336),\n",
       " ('monk', 0.5043931007385254),\n",
       " ('men', 0.49725764989852905)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12857, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words):\n",
    "    featureVec = np.zeros(shape=(word2vec_model.wv.vectors.shape[1],), dtype=\"float32\")\n",
    "    count = 0\n",
    "    word_set = set(word2vec_model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in word_set:\n",
    "            count += 1\n",
    "            featureVec += word2vec_model.wv[word]\n",
    "    return featureVec / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVec(sentences):\n",
    "    res = np.empty(shape=(len(sentences), word2vec_model.wv.vectors.shape[1]), dtype=\"float32\")\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        if index % 5000 == 0:\n",
    "            print(index)\n",
    "        res[index] = makeFeatureVec(sentence.split())\n",
    "        index += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_features = getAvgFeatureVec(train_data['words'])\n",
    "test_features = getAvgFeatureVec(test_data['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SGD:')\n",
    "run_cross_validation(train_features, train_data['sentiment'].values, SGDClassifier, SGD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(**SGD_params)\n",
    "model.fit(train_features, train_data['sentiment'].values)\n",
    "result = model.predict_proba(test_features)[:, 1]\n",
    "test_data['sentiment'] = result\n",
    "# Kaggle ROC-AUC score: 0.93756\n",
    "test_data[['id', 'sentiment']].to_csv('res/Word2Vec.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
