{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path ='./feature/'\n",
    "res_path = './res/'\n",
    "data_path = './data/'\n",
    "second_round_path = './second round/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(data_path+'transaction_train_new.csv')\n",
    "operation_df =  pd.read_csv(data_path+'operation_train_new.csv')\n",
    "label = pd.read_csv(data_path+'tag_train_new.csv')\n",
    "\n",
    "transaction_test = pd.read_csv(data_path+'transaction_round1_new.csv')\n",
    "operation_test = pd.read_csv(data_path+'operation_round1_new.csv')\n",
    "\n",
    "# transaction_test = pd.read_csv(second_round_path+'test_transaction_round2.csv')\n",
    "# operation_test = pd.read_csv(second_round_path+'test_operation_round2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_count(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].count().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_nunique(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].nunique().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_value_count(df1,df2,col, name=None, divisor=None):\n",
    "    if name is None:\n",
    "        name = col[1]\n",
    "    tmp = df1.groupby(col).size().reset_index().rename(columns = {0:'cnt'})\n",
    "    df = tmp.pivot(index=col[0],columns=col[1],values='cnt').reset_index()\n",
    "    if divisor is not None:\n",
    "        df = df.apply(lambda x: [x.iloc[0], *(x.iloc[1:] / divisor[x.iloc[0]])], axis=1, result_type='broadcast')\n",
    "    cname = [col[0]]\n",
    "    for index in range(1,len(df.columns)):\n",
    "        cname.append(str(name)+'_'+str(df.columns[index]))\n",
    "    df.columns=cname\n",
    "    df = df.fillna(0)\n",
    "    df2 = df2.merge(df,on=str(col[0]),how='left')\n",
    "    del df,df1\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "def gap(x):\n",
    "    valid = x.dropna()\n",
    "    if not valid.empty:\n",
    "        return max(valid) - min(valid)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_op_fea(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    #op_day\n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "    tmp = operation_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','op_day_max','op_day_min','op_day_mean', 'op_day_gap']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    #Geo info\n",
    "    geo = operation_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    operation_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    operation_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = operation_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_latitude_max', 'op_latitude_min', 'op_latitude_mean', 'op_latitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    tmp = operation_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_longtitude_max', 'op_longtitude_min', 'op_longtitude_mean', 'op_longtitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    #op_mode count\n",
    "    op_fea = merge_count(operation_df,op_fea,'UID','mode','op_cnt')\n",
    "    op_fea = merge_nunique(operation_df,op_fea,'UID','mode','op_mode_nunique')\n",
    "    #success count\n",
    "    op_fea = merge_count(operation_df[operation_df.success==0],op_fea,'UID','mode','op_fail_cnt')\n",
    "    op_fea = merge_count(operation_df[operation_df.success==1],op_fea,'UID','mode','op_success_cnt')\n",
    "    op_fea['op_fail_cnt'] = op_fea['op_fail_cnt'].fillna(0)\n",
    "    op_fea['op_success_cnt'] = op_fea['op_success_cnt'].fillna(0)\n",
    "    \n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','day'],'op_days', divisor=op_counts)\n",
    "    #op_time\n",
    "    operation_df['op_hour'] = operation_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = operation_df.groupby('UID')['op_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','op_hour_max','op_hour_min','op_hour_mean']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','op_hour'], divisor=op_counts)\n",
    "    \n",
    "    #op_os\n",
    "    for col in ['os','version','device1','device2','device_code1','device_code2','mac1','ip1','ip2','device_code3','mac2','wifi','ip1_sub','ip2_sub']:\n",
    "        op_fea = merge_nunique(operation_df,op_fea,'UID',col,'op_'+col+'_nunique')\n",
    "    return op_fea\n",
    "\n",
    "def get_trans_fea(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    \n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "    #trans_channel\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','channel'], 'trans_channel', divisor=trans_counts)\n",
    "    trans_fea = merge_count(transaction_df,trans_fea,'UID','channel','trans_cnt')\n",
    "    trans_fea = merge_nunique(transaction_df,trans_fea,'UID','channel','trans_channel_nunique')\n",
    "    \n",
    "    tmp = transaction_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','op_day_max','op_day_min','op_day_mean', 'op_day_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    \n",
    "    geo = transaction_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    transaction_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    transaction_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = transaction_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_latitude_max', 'trans_latitude_min', 'trans_latitude_mean', 'trans_latitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    tmp = transaction_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_longtitude_max', 'trans_longtitude_min', 'trans_longtitude_mean', 'trans_longtitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    \n",
    "    #trans time\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = transaction_df.groupby('UID')['trans_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','trans_hour_max','trans_hour_min','trans_hour_mean']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','trans_hour'], divisor=trans_counts)\n",
    "\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_value_count(transaction_df,trans_fea,['UID',col], 'trans_%s'%col, divisor=trans_counts)\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "    for col in ['trans_type1','merchant','code1','code2','acc_id1','device_code1','device_code2','device_code3','device1','device2','mac1','ip1','acc_id2','acc_id3','market_code','ip1_sub']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "    return trans_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_features = get_op_fea(operation_df)\n",
    "transaction_features = get_trans_fea(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_feat = get_op_fea(operation_test)\n",
    "transaction_test_feat = get_trans_fea(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = operation_features.merge(transaction_features, on='UID', how='outer')\n",
    "train_data = train_features.merge(label,on='UID',how='left')\n",
    "train_data = train_data.drop('UID', axis=1)\n",
    "test_features = operation_test_feat.merge(transaction_test_feat, on='UID', how='outer').sort_values(['UID'])\n",
    "UIDs = test_features['UID']\n",
    "test_features = test_features.drop('UID', axis=1)\n",
    "\n",
    "columns_to_drop = [col for col in train_data.columns if col not in test_features.columns and col != 'Tag']\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "columns_to_drop = [col for col in test_features.columns if col not in train_data.columns and col != 'Tag']\n",
    "test_features = test_features.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "def preprocess(x):\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_scorer(estimator, x, y_true):\n",
    "    y_predict = estimator.predict_proba(x)[:, 1]\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    \n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "def cv(x, y, params={}, splits=5):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'tpr': tpr_scorer,\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    })\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: [10.19614577 12.89588308 12.1800468  14.28875399 13.74047017]\n",
      "Average fit_time: 12.660260\n",
      "score_time: [0.79093027 0.81535888 0.8494072  0.91731119 0.85164523]\n",
      "Average score_time: 0.844931\n",
      "test_tpr: [0.78366394 0.75402567 0.74644107 0.73955659 0.76697783]\n",
      "Average test_tpr: 0.758133\n",
      "train_tpr: [1. 1. 1. 1. 1.]\n",
      "Average train_tpr: 1.000000\n",
      "test_accuracy: [0.96295702 0.9599102  0.95974984 0.95894804 0.96263031]\n",
      "Average test_accuracy: 0.960839\n",
      "train_accuracy: [0.99911799 0.99903781 0.99923826 0.99899771 0.99895767]\n",
      "Average train_accuracy: 0.999070\n",
      "test_f1: [0.96295702 0.9599102  0.95974984 0.95894804 0.96263031]\n",
      "Average test_f1: 0.960839\n",
      "train_f1: [0.99911799 0.99903781 0.99923826 0.99899771 0.99895767]\n",
      "Average train_f1: 0.999070\n",
      "test_roc_auc: [0.96684674 0.96742561 0.96684208 0.96278865 0.97022725]\n",
      "Average test_roc_auc: 0.966826\n",
      "train_roc_auc: [0.99999264 0.99999171 0.99999473 0.99999437 0.99999003]\n",
      "Average train_roc_auc: 0.999993\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "#     'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 80,\n",
    "#     'max_leaf_nodes': 50,\n",
    "    'min_child_samples': 400,\n",
    "    'n_estimators': 3000,\n",
    "    'learning_rate': 0.05,\n",
    "    'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "    'boost_from_average': True,\n",
    "    'min_child_weight': 4,\n",
    "#     'gamma': 3,\n",
    "    'reg_alpha': 3,\n",
    "    'reg_lambda': 10,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree':0.7, \n",
    "    'subsample_freq': 1,\n",
    "    'n_jobs': -1,\n",
    "#     'booster':'gbtree',\n",
    "}\n",
    "cv_result = cv(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values, params=params, splits=5)\n",
    "for scorer, score in cv_result.items():\n",
    "    print('%s: %s' % (scorer, score))\n",
    "    print('Average %s: %f' % (scorer, score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, params={}):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_proba(test_features.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame()\n",
    "result_frame['UID'] = UIDs\n",
    "result_frame['Tag'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame.to_csv(res_path + 'res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08670427591512277"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
