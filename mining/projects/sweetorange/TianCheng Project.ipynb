{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path ='./feature/'\n",
    "res_path = './res/'\n",
    "data_path = './data/'\n",
    "second_round_path = './second round/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(data_path+'transaction_train_new.csv')\n",
    "operation_df =  pd.read_csv(data_path+'operation_train_new.csv')\n",
    "label = pd.read_csv(data_path+'tag_train_new.csv')\n",
    "\n",
    "transaction_test = pd.read_csv(data_path+'transaction_round1_new.csv')\n",
    "operation_test = pd.read_csv(data_path+'operation_round1_new.csv')\n",
    "\n",
    "# transaction_test = pd.read_csv(second_round_path+'test_transaction_round2.csv')\n",
    "# operation_test = pd.read_csv(second_round_path+'test_operation_round2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete abnormal data\n",
    "abnormal_device1 = [name for name, count in operation_df.groupby('device1').size().iteritems() if count > 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation_df = operation_df[~operation_df.device1.isin(abnormal_device1)]\n",
    "# transaction_df = transaction_df[~transaction_df.device1.isin(abnormal_device1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_count(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].count().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_nunique(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].nunique().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_value_count(df1,df2,col, name=None, divisor=None):\n",
    "    if name is None:\n",
    "        name = col[1]\n",
    "    tmp = df1.groupby(col).size().reset_index().rename(columns = {0:'cnt'})\n",
    "    df = tmp.pivot(index=col[0],columns=col[1],values='cnt').reset_index()\n",
    "    if divisor is not None:\n",
    "        df = df.apply(lambda x: [x.iloc[0], *(x.iloc[1:] / divisor[x.iloc[0]])], axis=1, result_type='broadcast')\n",
    "    cname = [col[0]]\n",
    "    for index in range(1,len(df.columns)):\n",
    "        cname.append(str(name)+'_'+str(df.columns[index]))\n",
    "    df.columns=cname\n",
    "    df = df.fillna(0)\n",
    "    df2 = df2.merge(df,on=str(col[0]),how='left')\n",
    "    del df,df1\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for i, row in label.iterrows():\n",
    "    labels_dict[row['UID']] = row['Tag']\n",
    "    \n",
    "def prepare_ratio_data(transaction_df, label, name):\n",
    "    col = transaction_df.groupby('UID')[name]\n",
    "    category_sum = dict()\n",
    "    category_black_count = dict()\n",
    "    def manage_categories(m):\n",
    "        uid = m.name\n",
    "        m = m.unique()\n",
    "        for c_id in m:\n",
    "            category_sum[c_id] = category_sum.get(c_id, 0) + 1\n",
    "            if labels_dict[uid] == 1:\n",
    "                category_black_count[c_id] = category_black_count.get(c_id, 0) + 1\n",
    "    col.apply(manage_categories)\n",
    "        \n",
    "    return {m_id: category_black_count.get(m_id, 0) / m_count for m_id, m_count in category_sum.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_ratio = {\n",
    "    'merchant': prepare_ratio_data(transaction_df, label, 'merchant'),\n",
    "    'amt_src1': prepare_ratio_data(transaction_df, label, 'amt_src1'),\n",
    "    'trans_device2': prepare_ratio_data(transaction_df, label, 'device2'),\n",
    "    'op_device2': prepare_ratio_data(operation_df, label, 'device2'),\n",
    "    'acc_id2': prepare_ratio_data(transaction_df, label, 'acc_id2'),\n",
    "    'acc_id3': prepare_ratio_data(transaction_df, label, 'acc_id3'),\n",
    "}\n",
    "black_avg = {\n",
    "    k: np.mean([ratio.get(m, 0) for m in ratio.keys()]) for k, ratio in black_ratio.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_black_ratio(df, df_name, out_name):\n",
    "    df['%s_black_ratio'%out_name] = df[df_name].apply(lambda x: black_ratio[out_name].get(x, black_avg[out_name]))\n",
    "    tmp = df.groupby('UID')['%s_black_ratio'%out_name].agg([max, min, np.mean]).reset_index()\n",
    "    tmp.columns = ['UID', 'max_%s_black_ratio'%out_name, \"min_%s_black_ratio\"%out_name, \"avg_%s_black_ratio\"%out_name]\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPopularityCount(df, df_name, out_name):\n",
    "    categories = df[df_name].unique()\n",
    "    counts = {c: 0 for c in categories}\n",
    "    counts[np.nan] = np.nan\n",
    "    groups = df[['UID', df_name]].groupby('UID')\n",
    "    for uid, frame in groups:\n",
    "        for val in frame[df_name].unique():\n",
    "            counts[val] += 1\n",
    "    counts_list = [counts[v] for v in df[df_name]]\n",
    "    df[out_name] = counts_list\n",
    "    res = df.groupby('UID')[out_name].agg([max,min,np.mean]).reset_index()\n",
    "    res.columns = ['UID', '%s_max'%out_name, '%s_min'%out_name, '%s_avg'%out_name]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(day\n",
       " 1     2620\n",
       " 2     3092\n",
       " 3     3091\n",
       " 4     2869\n",
       " 5     3473\n",
       " 6     8619\n",
       " 7     6821\n",
       " 8     3637\n",
       " 9     2580\n",
       " 10    2731\n",
       " dtype: int64, day\n",
       " 1     17767\n",
       " 2     11241\n",
       " 3      3086\n",
       " 4      2826\n",
       " 5      4676\n",
       " 6      3309\n",
       " 7      3437\n",
       " 8     20130\n",
       " 9     10085\n",
       " 10     3304\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transaction_test.groupby('day').size()[:10],\n",
    " transaction_df.groupby('day').size()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weekday = lambda x: (x + 5) % 7\n",
    "test_weekday = lambda x: x % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week_day_features(dataframe, weekday_func):\n",
    "    days = dataframe['day'].apply(lambda x: weekday_func(x))\n",
    "    days = days.replace(0, 7)\n",
    "    dataframe['day_in_week'] = days\n",
    "    return dataframe\n",
    "operation_df = add_week_day_features(operation_df, train_weekday)\n",
    "transaction_df = add_week_day_features(transaction_df, train_weekday)\n",
    "operation_test = add_week_day_features(operation_test, test_weekday)\n",
    "transaction_test = add_week_day_features(transaction_test, test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "def gap(x):\n",
    "    valid = x.dropna()\n",
    "    if not valid.empty:\n",
    "        return max(valid) - min(valid)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_op_stats_fea(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    #op_day\n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "    tmp = operation_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','op_day_max','op_day_min','op_day_mean', 'op_day_gap']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    #Geo info\n",
    "    geo = operation_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    operation_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    operation_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = operation_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_latitude_max', 'op_latitude_min', 'op_latitude_mean', 'op_latitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    tmp = operation_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_longtitude_max', 'op_longtitude_min', 'op_longtitude_mean', 'op_longtitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    #op_mode count\n",
    "    op_fea = merge_count(operation_df,op_fea,'UID','mode','op_cnt')\n",
    "    op_fea = merge_nunique(operation_df,op_fea,'UID','mode','op_mode_nunique')\n",
    "    \n",
    "    #op_time\n",
    "    operation_df['op_hour'] = operation_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = operation_df.groupby('UID')['op_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','op_hour_max','op_hour_min','op_hour_mean']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "        \n",
    "    #op_os\n",
    "    for col in ['os','version','device1','device2','device_code1','device_code2','mac1','ip1','ip2','device_code3','mac2','wifi','ip1_sub','ip2_sub','geo_code']:\n",
    "        op_fea = merge_nunique(operation_df,op_fea,'UID',col,'op_'+col+'_nunique')\n",
    "    return op_fea\n",
    "\n",
    "def get_trans_stats_fea(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    \n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "        \n",
    "    tmp = transaction_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','trans_day_max','trans_day_min','trans_day_mean', 'trans_day_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    tmp = transaction_df.groupby('UID')['trans_amt'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','trans_amt_max','trans_amt_min','trans_amt_mean', 'trans_amt_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    geo = transaction_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    transaction_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    transaction_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = transaction_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_latitude_max', 'trans_latitude_min', 'trans_latitude_mean', 'trans_latitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    tmp = transaction_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_longtitude_max', 'trans_longtitude_min', 'trans_longtitude_mean', 'trans_longtitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    \n",
    "    #trans time\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = transaction_df.groupby('UID')['trans_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','trans_hour_max','trans_hour_min','trans_hour_mean']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    for col in ['trans_type1','merchant','code1','code2','acc_id1','device_code1','device_code2','device_code3','device1','device2','mac1','ip1','acc_id2','acc_id3','market_code','ip1_sub','geo_code']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "        \n",
    "    return trans_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_counted_features(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "#     trans_fea = merge_value_count(transaction_df,trans_fea,['UID','day'], 'trans_day', divisor=trans_counts)\n",
    "    #trans_channel\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','channel'], 'trans_channel', divisor=trans_counts)\n",
    "    trans_fea = merge_count(transaction_df,trans_fea,'UID','channel','trans_cnt')\n",
    "    trans_fea = merge_nunique(transaction_df,trans_fea,'UID','channel','trans_channel_nunique')\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','trans_hour'], divisor=trans_counts)\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_value_count(transaction_df,trans_fea,['UID',col], 'trans_%s'%col, divisor=trans_counts)\n",
    "        \n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','day_in_week'], 'trans_week_day', divisor=trans_counts)\n",
    "\n",
    "    trans_fea['trans_on_weekend'] = trans_fea[['trans_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    trans_fea['trans_on_weekday'] = trans_fea[['trans_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    pop_count = [\n",
    "        ('device2', 'trans_device2_pop')\n",
    "    ]\n",
    "    for df_name, out_name in pop_count:\n",
    "        pop = getPopularityCount(transaction_df, df_name, out_name)\n",
    "        trans_fea = pd.merge(trans_fea,pop,on='UID',how='left')\n",
    "    \n",
    "    ratio_list = [\n",
    "        ('merchant', 'merchant'),\n",
    "        ('amt_src1', 'amt_src1'),\n",
    "        ('device2', 'trans_device2'),\n",
    "        ('acc_id2', 'acc_id2'),\n",
    "        ('acc_id3', 'acc_id3')\n",
    "    ]\n",
    "    \n",
    "    for df_name, out_name in ratio_list:\n",
    "        ratios = get_black_ratio(transaction_df, df_name, out_name)\n",
    "        trans_fea = trans_fea.merge(ratios, on='UID', how='left')\n",
    "    return trans_fea\n",
    "    \n",
    "    \n",
    "def get_op_counted_features(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "#     op_fea = merge_value_count(operation_df,op_fea,['UID','day'],'op_days', divisor=op_counts)\n",
    "    #success count\n",
    "    op_fea = merge_count(operation_df[operation_df.success==0],op_fea,'UID','mode','op_fail_cnt')\n",
    "    op_fea = merge_count(operation_df[operation_df.success==1],op_fea,'UID','mode','op_success_cnt')\n",
    "    op_fea['op_fail_cnt'] = op_fea['op_fail_cnt'].fillna(0)\n",
    "    op_fea['op_success_cnt'] = op_fea['op_success_cnt'].fillna(0)\n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','op_hour'], divisor=op_counts) \n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','day_in_week'], 'op_week_day', divisor=op_counts) \n",
    "\n",
    "    op_fea['op_on_weekend'] = op_fea[['op_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    op_fea['op_on_weekday'] = op_fea[['op_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    pop_count = [\n",
    "        ('wifi', 'op_wifi_pop'),\n",
    "        ('os', 'op_os_pop'),\n",
    "        ('version', 'op_version_pop'),\n",
    "        ('device2', 'op_device2_pop')\n",
    "    ]\n",
    "    for df_name, out_name in pop_count:\n",
    "        pop = getPopularityCount(operation_df, df_name, out_name)\n",
    "        op_fea = pd.merge(op_fea,pop,on='UID',how='left')\n",
    "    \n",
    "    ratio_list = [\n",
    "        ('device2', 'op_device2'),\n",
    "    ]\n",
    "    \n",
    "    for df_name, out_name in ratio_list:\n",
    "        ratios = get_black_ratio(operation_df, df_name, out_name)\n",
    "        op_fea = op_fea.merge(ratios, on='UID', how='left')\n",
    "    \n",
    "    return op_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stats\n",
    "operation_features_stats = get_op_stats_fea(operation_df)\n",
    "transaction_features_stats = get_trans_stats_fea(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_stats = get_op_stats_fea(operation_test)\n",
    "transaction_test_stats = get_trans_stats_fea(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counted data\n",
    "operation_features_counted = get_op_counted_features(operation_df)\n",
    "transaction_features_counted = get_trans_counted_features(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_counted = get_op_counted_features(operation_test)\n",
    "transaction_test_counted = get_trans_counted_features(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = operation_features_stats.merge(transaction_features_stats, on='UID', how='outer')\n",
    "train_features = train_features.merge(operation_features_counted, on='UID', how='outer')\n",
    "train_features = train_features.merge(transaction_features_counted, on='UID', how='outer')\n",
    "train_data = train_features.merge(label,on='UID',how='left')\n",
    "train_data = train_data.drop('UID', axis=1)\n",
    "test_features = operation_test_stats.merge(transaction_test_stats, on='UID', how='outer').sort_values(['UID'])\n",
    "test_features = test_features.merge(operation_test_counted, on='UID', how='outer')\n",
    "test_features = test_features.merge(transaction_test_counted, on='UID', how='outer').sort_values(['UID'])\n",
    "\n",
    "UIDs = test_features['UID']\n",
    "test_features = test_features.drop('UID', axis=1)\n",
    "\n",
    "columns_to_drop = [col for col in train_data.columns if col not in test_features.columns and col != 'Tag']\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "columns_to_drop = [col for col in test_features.columns if col not in train_data.columns and col != 'Tag']\n",
    "test_features = test_features.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#Fill nan with average\n",
    "train_data = train_data.fillna(train_data.mean())\n",
    "test_features = test_features.fillna(train_data.drop('Tag', axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "def preprocess(x):\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_scorer(estimator, x, y_true):\n",
    "    y_predict = estimator.predict_proba(x)[:, 1]\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    \n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import lightgbm as lgb\n",
    "import sklearn.ensemble\n",
    "import xgboost as xgb\n",
    "\n",
    "def cv(x, y, params={}, splits=5):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'tpr': tpr_scorer,\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }, return_train_score=True)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        \"num_leaves\": 200,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        'min_child_samples': 100,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "        'boost_from_average': True,\n",
    "        'min_child_weight': 1e-3,\n",
    "        \"subsample_for_bin\": 20000,\n",
    "        'max_bin': 512,\n",
    "        \"metric\": 'auc',\n",
    "        'reg_alpha': 3,\n",
    "        'reg_lambda': 5,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree':0.7, \n",
    "        'subsample_freq': 1,\n",
    "        'n_jobs': -1,\n",
    "}\n",
    "# params = {\n",
    "#         'booster':'gbtree',\n",
    "#         'objective':'binary:logistic',\n",
    "#         'stratified':True,\n",
    "#         'max_depth':8,\n",
    "#         'min_child_weight':1,\n",
    "#         'gamma':3,\n",
    "#         'subsample':0.8,#0.7\n",
    "#         'colsample_bytree':0.6, \n",
    "#         'reg_lambda':3, \n",
    "#         'eta':0.05,\n",
    "#         'seed':20,\n",
    "#         'silent':1,\n",
    "#         'eval_metric':'auc',\n",
    "#         'n_jobs': 12,\n",
    "#         'n_estimators': 400,\n",
    "#         'early_stopping_round': 200,\n",
    "#         'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "#         'verbosity': 2\n",
    "# }\n",
    "\n",
    "def run_cross_validation(x, y):\n",
    "    cv_result = cv(x, y, params=params, splits=5)\n",
    "    for scorer, score in cv_result.items():\n",
    "        print('%s: %s' % (scorer, score))\n",
    "        print('Average %s: %f' % (scorer, score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: [ 9.86293077  9.6645081   9.2724762   9.21680212 10.15913916]\n",
      "Average fit_time: 9.635171\n",
      "score_time: [0.36270833 0.43045402 0.3891542  0.38878274 0.43438697]\n",
      "Average score_time: 0.401097\n",
      "test_tpr: [0.85939323 0.8722287  0.84527421 0.84492415 0.8134189 ]\n",
      "Average test_tpr: 0.847048\n",
      "train_tpr: [1. 1. 1. 1. 1.]\n",
      "Average train_tpr: 1.000000\n",
      "test_accuracy: [0.97594612 0.97674792 0.9764272  0.97305965 0.97161187]\n",
      "Average test_accuracy: 0.974759\n",
      "train_accuracy: [0.99963918 0.99983963 0.99971936 0.99967927 0.99975946]\n",
      "Average train_accuracy: 0.999727\n",
      "test_f1: [0.97594612 0.97674792 0.9764272  0.97305965 0.97161187]\n",
      "Average test_f1: 0.974759\n",
      "train_f1: [0.99963918 0.99983963 0.99971936 0.99967927 0.99975946]\n",
      "Average train_f1: 0.999727\n",
      "test_roc_auc: [0.99295588 0.99217385 0.99119876 0.99078637 0.99055619]\n",
      "Average test_roc_auc: 0.991534\n",
      "train_roc_auc: [0.99999992 0.99999995 0.99999997 0.99999999 0.99999997]\n",
      "Average train_roc_auc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation(train_data.drop('Tag', axis=1).fillna(train_data.drop('Tag', axis=1).mean()).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, params={}):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_proba(test_features.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09789089044169498"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_scorer(model, train_data.drop('Tag', axis=1).fillna(train_data.drop('Tag', axis=1).mean()).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame()\n",
    "result_frame['UID'] = UIDs\n",
    "result_frame['Tag'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_frame.to_csv(res_path + 'res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('op_day_max', 601),\n",
       " ('op_day_min', 521),\n",
       " ('op_day_mean', 2088),\n",
       " ('op_day_gap', 368),\n",
       " ('op_latitude_max', 501),\n",
       " ('op_latitude_min', 460),\n",
       " ('op_latitude_mean', 450),\n",
       " ('op_latitude_gap', 158),\n",
       " ('op_longtitude_max', 523),\n",
       " ('op_longtitude_min', 539),\n",
       " ('op_longtitude_mean', 488),\n",
       " ('op_longtitude_gap', 55),\n",
       " ('op_cnt', 1465),\n",
       " ('op_mode_nunique', 888),\n",
       " ('op_hour_max', 508),\n",
       " ('op_hour_min', 643),\n",
       " ('op_hour_mean', 1692),\n",
       " ('op_os_nunique', 66),\n",
       " ('op_version_nunique', 43),\n",
       " ('op_device1_nunique', 47),\n",
       " ('op_device2_nunique', 39),\n",
       " ('op_device_code1_nunique', 32),\n",
       " ('op_device_code2_nunique', 140),\n",
       " ('op_mac1_nunique', 69),\n",
       " ('op_ip1_nunique', 291),\n",
       " ('op_ip2_nunique', 58),\n",
       " ('op_device_code3_nunique', 57),\n",
       " ('op_mac2_nunique', 110),\n",
       " ('op_wifi_nunique', 168),\n",
       " ('op_ip1_sub_nunique', 396),\n",
       " ('op_ip2_sub_nunique', 36),\n",
       " ('op_geo_code_nunique', 66),\n",
       " ('trans_day_max', 427),\n",
       " ('trans_day_min', 432),\n",
       " ('trans_day_mean', 1223),\n",
       " ('trans_day_gap', 165),\n",
       " ('trans_amt_max', 557),\n",
       " ('trans_amt_min', 949),\n",
       " ('trans_amt_mean', 1385),\n",
       " ('trans_amt_gap', 845),\n",
       " ('trans_latitude_max', 377),\n",
       " ('trans_latitude_min', 335),\n",
       " ('trans_latitude_mean', 263),\n",
       " ('trans_latitude_gap', 29),\n",
       " ('trans_longtitude_max', 347),\n",
       " ('trans_longtitude_min', 288),\n",
       " ('trans_longtitude_mean', 206),\n",
       " ('trans_longtitude_gap', 13),\n",
       " ('trans_hour_max', 495),\n",
       " ('trans_hour_min', 579),\n",
       " ('trans_hour_mean', 1120),\n",
       " ('trans_trans_type1_nunique', 61),\n",
       " ('trans_merchant_nunique', 250),\n",
       " ('trans_code1_nunique', 37),\n",
       " ('trans_code2_nunique', 0),\n",
       " ('trans_acc_id1_nunique', 107),\n",
       " ('trans_device_code1_nunique', 43),\n",
       " ('trans_device_code2_nunique', 100),\n",
       " ('trans_device_code3_nunique', 39),\n",
       " ('trans_device1_nunique', 30),\n",
       " ('trans_device2_nunique', 40),\n",
       " ('trans_mac1_nunique', 39),\n",
       " ('trans_ip1_nunique', 209),\n",
       " ('trans_acc_id2_nunique', 16),\n",
       " ('trans_acc_id3_nunique', 4),\n",
       " ('trans_market_code_nunique', 86),\n",
       " ('trans_ip1_sub_nunique', 290),\n",
       " ('trans_geo_code_nunique', 49),\n",
       " ('trans_trans_type2_nunique', 56),\n",
       " ('trans_market_type_nunique', 20),\n",
       " ('op_fail_cnt', 526),\n",
       " ('op_success_cnt', 1242),\n",
       " ('op_hour_0', 64),\n",
       " ('op_hour_1', 34),\n",
       " ('op_hour_2', 25),\n",
       " ('op_hour_3', 27),\n",
       " ('op_hour_4', 8),\n",
       " ('op_hour_5', 42),\n",
       " ('op_hour_6', 99),\n",
       " ('op_hour_7', 247),\n",
       " ('op_hour_8', 356),\n",
       " ('op_hour_9', 562),\n",
       " ('op_hour_10', 592),\n",
       " ('op_hour_11', 531),\n",
       " ('op_hour_12', 469),\n",
       " ('op_hour_13', 453),\n",
       " ('op_hour_14', 533),\n",
       " ('op_hour_15', 524),\n",
       " ('op_hour_16', 556),\n",
       " ('op_hour_17', 508),\n",
       " ('op_hour_18', 456),\n",
       " ('op_hour_19', 351),\n",
       " ('op_hour_20', 365),\n",
       " ('op_hour_21', 361),\n",
       " ('op_hour_22', 177),\n",
       " ('op_hour_23', 155),\n",
       " ('op_week_day_1', 432),\n",
       " ('op_week_day_2', 377),\n",
       " ('op_week_day_3', 486),\n",
       " ('op_week_day_4', 499),\n",
       " ('op_week_day_5', 417),\n",
       " ('op_week_day_6', 498),\n",
       " ('op_week_day_7', 533),\n",
       " ('op_on_weekend', 452),\n",
       " ('op_on_weekday', 222),\n",
       " ('op_wifi_pop_max', 595),\n",
       " ('op_wifi_pop_min', 388),\n",
       " ('op_wifi_pop_avg', 731),\n",
       " ('op_os_pop_max', 3),\n",
       " ('op_os_pop_min', 136),\n",
       " ('op_os_pop_avg', 753),\n",
       " ('op_version_pop_max', 85),\n",
       " ('op_version_pop_min', 110),\n",
       " ('op_version_pop_avg', 880),\n",
       " ('op_device2_pop_max', 744),\n",
       " ('op_device2_pop_min', 723),\n",
       " ('op_device2_pop_avg', 704),\n",
       " ('max_op_device2_black_ratio', 423),\n",
       " ('min_op_device2_black_ratio', 1399),\n",
       " ('avg_op_device2_black_ratio', 2026),\n",
       " ('trans_channel_102', 57),\n",
       " ('trans_channel_106', 0),\n",
       " ('trans_channel_118', 13),\n",
       " ('trans_channel_119', 0),\n",
       " ('trans_channel_140', 122),\n",
       " ('trans_cnt', 666),\n",
       " ('trans_channel_nunique', 67),\n",
       " ('trans_hour_0', 19),\n",
       " ('trans_hour_1', 2),\n",
       " ('trans_hour_2', 0),\n",
       " ('trans_hour_3', 3),\n",
       " ('trans_hour_4', 6),\n",
       " ('trans_hour_5', 0),\n",
       " ('trans_hour_6', 14),\n",
       " ('trans_hour_7', 86),\n",
       " ('trans_hour_8', 105),\n",
       " ('trans_hour_9', 142),\n",
       " ('trans_hour_10', 231),\n",
       " ('trans_hour_11', 118),\n",
       " ('trans_hour_12', 140),\n",
       " ('trans_hour_13', 161),\n",
       " ('trans_hour_14', 123),\n",
       " ('trans_hour_15', 169),\n",
       " ('trans_hour_16', 180),\n",
       " ('trans_hour_17', 152),\n",
       " ('trans_hour_18', 179),\n",
       " ('trans_hour_19', 140),\n",
       " ('trans_hour_20', 100),\n",
       " ('trans_hour_21', 91),\n",
       " ('trans_hour_22', 21),\n",
       " ('trans_hour_23', 1),\n",
       " ('trans_trans_type2_102.0', 58),\n",
       " ('trans_trans_type2_103.0', 3),\n",
       " ('trans_trans_type2_104.0', 11),\n",
       " ('trans_trans_type2_105.0', 199),\n",
       " ('trans_market_type_1.0', 16),\n",
       " ('trans_market_type_2.0', 111),\n",
       " ('trans_week_day_1', 82),\n",
       " ('trans_week_day_2', 36),\n",
       " ('trans_week_day_3', 70),\n",
       " ('trans_week_day_4', 95),\n",
       " ('trans_week_day_5', 55),\n",
       " ('trans_week_day_6', 82),\n",
       " ('trans_week_day_7', 71),\n",
       " ('trans_on_weekend', 80),\n",
       " ('trans_on_weekday', 34),\n",
       " ('trans_device2_pop_max', 701),\n",
       " ('trans_device2_pop_min', 680),\n",
       " ('trans_device2_pop_avg', 553),\n",
       " ('max_merchant_black_ratio', 1848),\n",
       " ('min_merchant_black_ratio', 1782),\n",
       " ('avg_merchant_black_ratio', 1840),\n",
       " ('max_amt_src1_black_ratio', 317),\n",
       " ('min_amt_src1_black_ratio', 225),\n",
       " ('avg_amt_src1_black_ratio', 1550),\n",
       " ('max_trans_device2_black_ratio', 976),\n",
       " ('min_trans_device2_black_ratio', 1261),\n",
       " ('avg_trans_device2_black_ratio', 880),\n",
       " ('max_acc_id2_black_ratio', 1),\n",
       " ('min_acc_id2_black_ratio', 9),\n",
       " ('avg_acc_id2_black_ratio', 50),\n",
       " ('max_acc_id3_black_ratio', 41),\n",
       " ('min_acc_id3_black_ratio', 58),\n",
       " ('avg_acc_id3_black_ratio', 160)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(train_data.drop('Tag', axis=1).columns[i], model.feature_importances_[i]) for i in range(len(model.feature_importances_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
