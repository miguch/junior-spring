{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path ='./feature/'\n",
    "res_path = './res/'\n",
    "data_path = './data/'\n",
    "second_round_path = './second round/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(data_path+'transaction_train_new.csv')\n",
    "operation_df =  pd.read_csv(data_path+'operation_train_new.csv')\n",
    "label = pd.read_csv(data_path+'tag_train_new.csv')\n",
    "\n",
    "transaction_test = pd.read_csv(data_path+'transaction_round1_new.csv')\n",
    "operation_test = pd.read_csv(data_path+'operation_round1_new.csv')\n",
    "\n",
    "# transaction_test = pd.read_csv(second_round_path+'test_transaction_round2.csv')\n",
    "# operation_test = pd.read_csv(second_round_path+'test_operation_round2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete abnormal data\n",
    "abnormal_device1 = [name for name, count in operation_df.groupby('device1').size().iteritems() if count > 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation_df = operation_df[~operation_df.device1.isin(abnormal_device1)]\n",
    "# transaction_df = transaction_df[~transaction_df.device1.isin(abnormal_device1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_count(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].count().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_nunique(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].nunique().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_value_count(df1,df2,col, name=None, divisor=None):\n",
    "    if name is None:\n",
    "        name = col[1]\n",
    "    tmp = df1.groupby(col).size().reset_index().rename(columns = {0:'cnt'})\n",
    "    df = tmp.pivot(index=col[0],columns=col[1],values='cnt').reset_index()\n",
    "    if divisor is not None:\n",
    "        df = df.apply(lambda x: [x.iloc[0], *(x.iloc[1:] / divisor[x.iloc[0]])], axis=1, result_type='broadcast')\n",
    "    cname = [col[0]]\n",
    "    for index in range(1,len(df.columns)):\n",
    "        cname.append(str(name)+'_'+str(df.columns[index]))\n",
    "    df.columns=cname\n",
    "    df = df.fillna(0)\n",
    "    df2 = df2.merge(df,on=str(col[0]),how='left')\n",
    "    del df,df1\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for i, row in label.iterrows():\n",
    "    labels_dict[row['UID']] = row['Tag']\n",
    "    \n",
    "def prepare_ratio_data(transaction_df, label, name):\n",
    "    col = transaction_df.groupby('UID')[name]\n",
    "    category_sum = dict()\n",
    "    category_black_count = dict()\n",
    "    def manage_categories(m):\n",
    "        uid = m.name\n",
    "        m = m.unique()\n",
    "        for c_id in m:\n",
    "            category_sum[c_id] = category_sum.get(c_id, 0) + 1\n",
    "            if labels_dict[uid] == 1:\n",
    "                category_black_count[c_id] = category_black_count.get(c_id, 0) + 1\n",
    "    col.apply(manage_categories)\n",
    "        \n",
    "    return {m_id: category_black_count.get(m_id, 0) / m_count for m_id, m_count in category_sum.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_ratio = {\n",
    "    'merchant': prepare_ratio_data(transaction_df, label, 'merchant'),\n",
    "    'amt_src1': prepare_ratio_data(transaction_df, label, 'amt_src1'),\n",
    "    'trans_device2': prepare_ratio_data(transaction_df, label, 'device2'),\n",
    "    'op_device2': prepare_ratio_data(operation_df, label, 'device2'),\n",
    "    'acc_id2': prepare_ratio_data(transaction_df, label, 'acc_id2'),\n",
    "    'acc_id2': prepare_ratio_data(transaction_df, label, 'acc_id3'),\n",
    "}\n",
    "black_avg = {\n",
    "    k: np.mean([ratio.get(m, 0) for m in ratio.keys()]) for k, ratio in black_ratio.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_black_ratio(df, df_name, out_name):\n",
    "    df['%s_black_ratio'%out_name] = df[df_name].apply(lambda x: black_ratio[out_name].get(x, black_avg[out_name]))\n",
    "    tmp = df.groupby('UID')['%s_black_ratio'%out_name].agg([max, min, np.mean]).reset_index()\n",
    "    tmp.columns = ['UID', 'max_%s_black_ratio'%out_name, \"min_%s_black_ratio\"%out_name, \"avg_%s_black_ratio\"%out_name]\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(day\n",
       " 1     2620\n",
       " 2     3092\n",
       " 3     3091\n",
       " 4     2869\n",
       " 5     3473\n",
       " 6     8619\n",
       " 7     6821\n",
       " 8     3637\n",
       " 9     2580\n",
       " 10    2731\n",
       " dtype: int64, day\n",
       " 1     17767\n",
       " 2     11241\n",
       " 3      3086\n",
       " 4      2826\n",
       " 5      4676\n",
       " 6      3309\n",
       " 7      3437\n",
       " 8     20130\n",
       " 9     10085\n",
       " 10     3304\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transaction_test.groupby('day').size()[:10],\n",
    " transaction_df.groupby('day').size()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weekday = lambda x: (x + 5) % 7\n",
    "test_weekday = lambda x: x % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week_day_features(dataframe, weekday_func):\n",
    "    days = dataframe['day'].apply(lambda x: weekday_func(x))\n",
    "    days = days.replace(0, 7)\n",
    "    dataframe['day_in_week'] = days\n",
    "    return dataframe\n",
    "operation_df = add_week_day_features(operation_df, train_weekday)\n",
    "transaction_df = add_week_day_features(transaction_df, train_weekday)\n",
    "operation_test = add_week_day_features(operation_test, test_weekday)\n",
    "transaction_test = add_week_day_features(transaction_test, test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "def gap(x):\n",
    "    valid = x.dropna()\n",
    "    if not valid.empty:\n",
    "        return max(valid) - min(valid)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_op_stats_fea(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    #op_day\n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "    tmp = operation_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','op_day_max','op_day_min','op_day_mean', 'op_day_gap']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    #Geo info\n",
    "    geo = operation_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    operation_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    operation_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = operation_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_latitude_max', 'op_latitude_min', 'op_latitude_mean', 'op_latitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    tmp = operation_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_longtitude_max', 'op_longtitude_min', 'op_longtitude_mean', 'op_longtitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    #op_mode count\n",
    "    op_fea = merge_count(operation_df,op_fea,'UID','mode','op_cnt')\n",
    "    op_fea = merge_nunique(operation_df,op_fea,'UID','mode','op_mode_nunique')\n",
    "    \n",
    "    #op_time\n",
    "    operation_df['op_hour'] = operation_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = operation_df.groupby('UID')['op_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','op_hour_max','op_hour_min','op_hour_mean']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "        \n",
    "    #op_os\n",
    "    for col in ['os','version','device1','device2','device_code1','device_code2','mac1','ip1','ip2','device_code3','mac2','wifi','ip1_sub','ip2_sub','geo_code']:\n",
    "        op_fea = merge_nunique(operation_df,op_fea,'UID',col,'op_'+col+'_nunique')\n",
    "    return op_fea\n",
    "\n",
    "def get_trans_stats_fea(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    \n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "        \n",
    "    tmp = transaction_df.groupby('UID')['day'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','trans_day_max','trans_day_min','trans_day_mean', 'trans_day_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    tmp = transaction_df.groupby('UID')['trans_amt'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns=['UID','trans_amt_max','trans_amt_min','trans_amt_mean', 'trans_amt_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    geo = transaction_df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    transaction_df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    transaction_df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "    tmp = transaction_df.groupby('UID')['latitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_latitude_max', 'trans_latitude_min', 'trans_latitude_mean', 'trans_latitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    tmp = transaction_df.groupby('UID')['longtitude'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_longtitude_max', 'trans_longtitude_min', 'trans_longtitude_mean', 'trans_longtitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    \n",
    "    #trans time\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    tmp = transaction_df.groupby('UID')['trans_hour'].agg([max,min,np.mean]).reset_index()\n",
    "    tmp.columns=['UID','trans_hour_max','trans_hour_min','trans_hour_mean']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    for col in ['trans_type1','merchant','code1','code2','acc_id1','device_code1','device_code2','device_code3','device1','device2','mac1','ip1','acc_id2','acc_id3','market_code','ip1_sub','geo_code']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "        \n",
    "    return trans_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_counted_features(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "#     trans_fea = merge_value_count(transaction_df,trans_fea,['UID','day'], 'trans_day', divisor=trans_counts)\n",
    "    #trans_channel\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','channel'], 'trans_channel', divisor=trans_counts)\n",
    "    trans_fea = merge_count(transaction_df,trans_fea,'UID','channel','trans_cnt')\n",
    "    trans_fea = merge_nunique(transaction_df,trans_fea,'UID','channel','trans_channel_nunique')\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','trans_hour'], divisor=trans_counts)\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_value_count(transaction_df,trans_fea,['UID',col], 'trans_%s'%col, divisor=trans_counts)\n",
    "        \n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','day_in_week'], 'trans_week_day', divisor=trans_counts)\n",
    "\n",
    "    trans_fea['trans_on_weekend'] = trans_fea[['trans_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    trans_fea['trans_on_weekday'] = trans_fea[['trans_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    # Merchants\n",
    "    merchant_ratio = get_black_ratio(transaction_df, 'merchant', 'merchant')\n",
    "    trans_fea = trans_fea.merge(merchant_ratio, on='UID', how='left')\n",
    "    \n",
    "    # amt_src\n",
    "    amt_src_ratio = get_black_ratio(transaction_df, 'amt_src1', 'amt_src1')\n",
    "    trans_fea = trans_fea.merge(amt_src_ratio, on='UID', how='left')\n",
    "    \n",
    "    device_2_ratio = get_black_ratio(transaction_df, 'device2', 'trans_device2')\n",
    "    trans_fea = trans_fea.merge(device_2_ratio, on='UID', how='left')\n",
    "    return trans_fea\n",
    "    \n",
    "    \n",
    "def get_op_counted_features(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "#     op_fea = merge_value_count(operation_df,op_fea,['UID','day'],'op_days', divisor=op_counts)\n",
    "    #success count\n",
    "    op_fea = merge_count(operation_df[operation_df.success==0],op_fea,'UID','mode','op_fail_cnt')\n",
    "    op_fea = merge_count(operation_df[operation_df.success==1],op_fea,'UID','mode','op_success_cnt')\n",
    "    op_fea['op_fail_cnt'] = op_fea['op_fail_cnt'].fillna(0)\n",
    "    op_fea['op_success_cnt'] = op_fea['op_success_cnt'].fillna(0)\n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','op_hour'], divisor=op_counts) \n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','day_in_week'], 'op_week_day', divisor=op_counts) \n",
    "\n",
    "    op_fea['op_on_weekend'] = op_fea[['op_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    op_fea['op_on_weekday'] = op_fea[['op_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    device_2_ratio = get_black_ratio(operation_df, 'device2', 'op_device2')\n",
    "    op_fea = op_fea.merge(device_2_ratio, on='UID', how='left')\n",
    "    \n",
    "    return op_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stats\n",
    "operation_features_stats = get_op_stats_fea(operation_df)\n",
    "transaction_features_stats = get_trans_stats_fea(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_stats = get_op_stats_fea(operation_test)\n",
    "transaction_test_stats = get_trans_stats_fea(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counted data\n",
    "operation_features_counted = get_op_counted_features(operation_df)\n",
    "transaction_features_counted = get_trans_counted_features(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_counted = get_op_counted_features(operation_test)\n",
    "transaction_test_counted = get_trans_counted_features(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = operation_features_stats.merge(transaction_features_stats, on='UID', how='outer')\n",
    "train_features = train_features.merge(operation_features_counted, on='UID', how='outer')\n",
    "train_features = train_features.merge(transaction_features_counted, on='UID', how='outer')\n",
    "train_data = train_features.merge(label,on='UID',how='left')\n",
    "train_data = train_data.drop('UID', axis=1)\n",
    "test_features = operation_test_stats.merge(transaction_test_stats, on='UID', how='outer').sort_values(['UID'])\n",
    "test_features = test_features.merge(operation_test_counted, on='UID', how='outer')\n",
    "test_features = test_features.merge(transaction_test_counted, on='UID', how='outer').sort_values(['UID'])\n",
    "\n",
    "UIDs = test_features['UID']\n",
    "test_features = test_features.drop('UID', axis=1)\n",
    "\n",
    "columns_to_drop = [col for col in train_data.columns if col not in test_features.columns and col != 'Tag']\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "columns_to_drop = [col for col in test_features.columns if col not in train_data.columns and col != 'Tag']\n",
    "test_features = test_features.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "def preprocess(x):\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_scorer(estimator, x, y_true):\n",
    "    y_predict = estimator.predict_proba(x)[:, 1]\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    \n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "def cv(x, y, params={}, splits=5):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'tpr': tpr_scorer,\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }, return_train_score=True)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        \"num_leaves\": 200,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        'min_child_samples': 20,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "        'boost_from_average': True,\n",
    "        'min_child_weight': 1e-3,\n",
    "        \"subsample_for_bin\": 20000,\n",
    "        'max_bin': 512,\n",
    "        \"metric\": 'auc',\n",
    "        'reg_alpha': 3,\n",
    "        'reg_lambda': 5,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree':0.7, \n",
    "        'subsample_freq': 1,\n",
    "        'n_jobs': -1,\n",
    "}\n",
    "# params = {\n",
    "#         'booster':'gbtree',\n",
    "#         'objective':'binary:logistic',\n",
    "#         'stratified':True,\n",
    "#         'max_depth':8,\n",
    "#         'min_child_weight':1,\n",
    "#         'gamma':3,\n",
    "#         'subsample':0.8,#0.7\n",
    "#         'colsample_bytree':0.6, \n",
    "#         'reg_lambda':3, \n",
    "#         'eta':0.05,\n",
    "#         'seed':20,\n",
    "#         'silent':1,\n",
    "#         'eval_metric':'auc',\n",
    "#         'n_jobs': 12,\n",
    "#         'n_estimators': 400,\n",
    "#         'early_stopping_round': 200,\n",
    "#         'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "#         'verbosity': 2\n",
    "# }\n",
    "\n",
    "def run_cross_validation(x, y):\n",
    "    cv_result = cv(x, y, params=params, splits=5)\n",
    "    for scorer, score in cv_result.items():\n",
    "        print('%s: %s' % (scorer, score))\n",
    "        print('Average %s: %f' % (scorer, score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: [12.79533291 14.62386918 13.91181064 17.49400306 15.91140008]\n",
      "Average fit_time: 14.947283\n",
      "score_time: [0.32064223 0.37739873 0.42323303 0.37701797 0.3750248 ]\n",
      "Average score_time: 0.374663\n",
      "test_tpr: [0.85752625 0.84504084 0.85005834 0.8453909  0.84107351]\n",
      "Average test_tpr: 0.847818\n",
      "train_tpr: [1. 1. 1. 1. 1.]\n",
      "Average train_tpr: 1.000000\n",
      "test_accuracy: [0.9764272  0.97434253 0.97257858 0.97434253 0.97497995]\n",
      "Average test_accuracy: 0.974534\n",
      "train_accuracy: [0.99967927 0.99983963 0.99971936 0.99963918 0.99967928]\n",
      "Average train_accuracy: 0.999711\n",
      "test_f1: [0.9764272  0.97434253 0.97257858 0.97434253 0.97497995]\n",
      "Average test_f1: 0.974534\n",
      "train_f1: [0.99967927 0.99983963 0.99971936 0.99963918 0.99967928]\n",
      "Average train_f1: 0.999711\n",
      "test_roc_auc: [0.99084473 0.9900486  0.99266476 0.99068637 0.99164364]\n",
      "Average test_roc_auc: 0.991178\n",
      "train_roc_auc: [0.99999997 0.99999998 0.99999997 0.99999999 0.99999998]\n",
      "Average train_roc_auc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, params={}):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_proba(test_features.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09654465029809603"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_scorer(model, train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame()\n",
    "result_frame['UID'] = UIDs\n",
    "result_frame['Tag'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_frame.to_csv(res_path + 'res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
