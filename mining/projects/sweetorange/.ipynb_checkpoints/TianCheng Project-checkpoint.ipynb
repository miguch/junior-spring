{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path ='./feature/'\n",
    "res_path = './res/'\n",
    "data_path = './data/'\n",
    "second_round_path = './second round/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(data_path+'transaction_train_new.csv')\n",
    "operation_df =  pd.read_csv(data_path+'operation_train_new.csv')\n",
    "label = pd.read_csv(data_path+'tag_train_new.csv')\n",
    "\n",
    "transaction_test = pd.read_csv(data_path+'transaction_round1_new.csv')\n",
    "operation_test = pd.read_csv(data_path+'operation_round1_new.csv')\n",
    "\n",
    "# transaction_test = pd.read_csv(second_round_path+'test_transaction_round2.csv')\n",
    "# operation_test = pd.read_csv(second_round_path+'test_operation_round2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete abnormal data\n",
    "abnormal_device1 = [name for name, count in operation_df.groupby('device1').size().iteritems() if count > 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation_df = operation_df[~operation_df.device1.isin(abnormal_device1)]\n",
    "# transaction_df = transaction_df[~transaction_df.device1.isin(abnormal_device1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_count(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].count().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_nunique(df1,df2,columns,value,cname):\n",
    "    add = df1.groupby(columns)[value].nunique().reset_index().rename(columns = {value:cname})\n",
    "    df2=df2.merge(add,on=columns,how=\"left\")\n",
    "    del add\n",
    "    return df2\n",
    "\n",
    "def merge_value_count(df1,df2,col, name=None, divisor=None):\n",
    "    if name is None:\n",
    "        name = col[1]\n",
    "    tmp = df1.groupby(col).size().reset_index().rename(columns = {0:'cnt'})\n",
    "    df = tmp.pivot(index=col[0],columns=col[1],values='cnt').reset_index()\n",
    "    if divisor is not None:\n",
    "        df = df.apply(lambda x: [x.iloc[0], *(x.iloc[1:] / divisor[x.iloc[0]])], axis=1, result_type='broadcast')\n",
    "    cname = [col[0]]\n",
    "    for index in range(1,len(df.columns)):\n",
    "        cname.append(str(name)+'_'+str(df.columns[index]))\n",
    "    df.columns=cname\n",
    "    df = df.fillna(0)\n",
    "    df2 = df2.merge(df,on=str(col[0]),how='left')\n",
    "    del df,df1\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "#Geo info\n",
    "def putGeoInfo(df):\n",
    "    geo = df['geo_code'].apply(lambda x: ((x, x) if pd.isna(x) else geohash.decode(x)))\n",
    "    df['latitude'] = geo.apply(lambda x: x[0])\n",
    "    df['longtitude'] = geo.apply(lambda x: x[1])\n",
    "\n",
    "dfs = [operation_df, operation_test, transaction_df, transaction_test]\n",
    "\n",
    "\n",
    "for df in dfs:\n",
    "    putGeoInfo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "def putGeoCluster(df, test_df):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=34, n_jobs=-1)\n",
    "    kmeans.fit(df[['latitude', 'longtitude']].fillna(-1))\n",
    "    df['geo_cluster'] = kmeans.predict(df[['latitude', 'longtitude']].fillna(-1))\n",
    "    test_df['geo_cluster'] = kmeans.predict(test_df[['latitude', 'longtitude']].fillna(-1))\n",
    "    \n",
    "putGeoCluster(operation_df, operation_test)\n",
    "putGeoCluster(transaction_df, transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for i, row in label.iterrows():\n",
    "    labels_dict[row['UID']] = row['Tag']\n",
    "    \n",
    "def prepare_ratio_data(transaction_df, label, name):\n",
    "    col = transaction_df.groupby('UID')[name]\n",
    "    category_sum = dict()\n",
    "    category_black_count = dict()\n",
    "    def manage_categories(m):\n",
    "        uid = m.name\n",
    "        m = m.unique()\n",
    "        for c_id in m:\n",
    "            category_sum[c_id] = category_sum.get(c_id, 0) + 1\n",
    "            if labels_dict[uid] == 1:\n",
    "                category_black_count[c_id] = category_black_count.get(c_id, 0) + 1\n",
    "    col.apply(manage_categories)\n",
    "        \n",
    "    return {m_id: category_black_count.get(m_id, 0) / m_count for m_id, m_count in category_sum.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_ratio = {\n",
    "    'merchant': prepare_ratio_data(transaction_df, label, 'merchant'),\n",
    "    'amt_src1': prepare_ratio_data(transaction_df, label, 'amt_src1'),\n",
    "    'trans_device2': prepare_ratio_data(transaction_df, label, 'device2'),\n",
    "    'op_device2': prepare_ratio_data(operation_df, label, 'device2'),\n",
    "#     'acc_id2': prepare_ratio_data(transaction_df, label, 'acc_id2'),\n",
    "    'acc_id3': prepare_ratio_data(transaction_df, label, 'acc_id3'),\n",
    "    'op_geo': prepare_ratio_data(operation_df, label, 'geo_cluster'),\n",
    "    'trans_geo': prepare_ratio_data(transaction_df, label, 'geo_cluster'),\n",
    "}\n",
    "black_avg = {\n",
    "    k: np.mean([ratio.get(m, 0) for m in ratio.keys()]) for k, ratio in black_ratio.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_black_ratio(df, df_name, out_name):\n",
    "    df['%s_black_ratio'%out_name] = df[df_name].apply(lambda x: black_ratio[out_name].get(x, black_avg[out_name]))\n",
    "    tmp = df.groupby('UID')['%s_black_ratio'%out_name].agg([max, min, np.mean]).reset_index()\n",
    "    tmp = pd.merge(tmp, df.groupby('UID')['%s_black_ratio'%out_name].skew().fillna(0), on='UID')\n",
    "    tmp.columns = ['UID', 'max_%s_black_ratio'%out_name, \"min_%s_black_ratio\"%out_name, \"avg_%s_black_ratio\"%out_name, 'skew_%s_black_ratio'%out_name]\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPopularityCount(df, df_name, out_name):\n",
    "    categories = df[df_name].unique()\n",
    "    counts = {c: 0 for c in categories}\n",
    "    counts[np.nan] = np.nan\n",
    "    groups = df[['UID', df_name]].groupby('UID')\n",
    "    for uid, frame in groups:\n",
    "        for val in frame[df_name].unique():\n",
    "            counts[val] += 1\n",
    "    counts_list = [counts[v] for v in df[df_name]]\n",
    "    df[out_name] = counts_list\n",
    "    res = df.groupby('UID')[out_name].agg([max,min,np.mean]).reset_index()\n",
    "    res = pd.merge(res, df.groupby('UID')[out_name].skew().fillna(0), on='UID')\n",
    "    res.columns = ['UID', '%s_max'%out_name, '%s_min'%out_name, '%s_avg'%out_name, '%s_skew'%out_name]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(day\n",
       " 1     2620\n",
       " 2     3092\n",
       " 3     3091\n",
       " 4     2869\n",
       " 5     3473\n",
       " 6     8619\n",
       " 7     6821\n",
       " 8     3637\n",
       " 9     2580\n",
       " 10    2731\n",
       " dtype: int64, day\n",
       " 1     17767\n",
       " 2     11241\n",
       " 3      3086\n",
       " 4      2826\n",
       " 5      4676\n",
       " 6      3309\n",
       " 7      3437\n",
       " 8     20130\n",
       " 9     10085\n",
       " 10     3304\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transaction_test.groupby('day').size()[:10],\n",
    " transaction_df.groupby('day').size()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weekday = lambda x: (x + 5) % 7\n",
    "test_weekday = lambda x: x % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week_day_features(dataframe, weekday_func):\n",
    "    days = dataframe['day'].apply(lambda x: weekday_func(x))\n",
    "    days = days.replace(0, 7)\n",
    "    dataframe['day_in_week'] = days\n",
    "    return dataframe\n",
    "operation_df = add_week_day_features(operation_df, train_weekday)\n",
    "transaction_df = add_week_day_features(transaction_df, train_weekday)\n",
    "operation_test = add_week_day_features(operation_test, test_weekday)\n",
    "transaction_test = add_week_day_features(transaction_test, test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap(x):\n",
    "    valid = x.dropna()\n",
    "    if not valid.empty:\n",
    "        return max(valid) - min(valid)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_op_stats_fea(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    #op_day\n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "    tmp = operation_df.groupby('UID')['day'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns=['UID','op_day_max','op_day_min', 'op_day_gap']\n",
    "    op_fea = pd.merge(op_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    \n",
    "    tmp = operation_df.groupby('UID')['latitude'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_latitude_max', 'op_latitude_min', 'op_latitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    tmp = operation_df.groupby('UID')['longtitude'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'op_longtitude_max', 'op_longtitude_min', 'op_longtitude_gap']\n",
    "    op_fea = pd.merge(op_fea, tmp, on='UID', how='left')\n",
    "    #op_mode count\n",
    "    op_fea = merge_count(operation_df,op_fea,'UID','mode','op_cnt')\n",
    "    op_fea = merge_nunique(operation_df,op_fea,'UID','mode','op_mode_nunique')\n",
    "    \n",
    "    #op_time\n",
    "    operation_df['op_hour'] = operation_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "        \n",
    "    #op_os\n",
    "    for col in ['os','version','device1','device2','device_code1','device_code2','mac1','ip1','ip2','device_code3','mac2','wifi','ip1_sub','ip2_sub','geo_code']:\n",
    "        op_fea = merge_nunique(operation_df,op_fea,'UID',col,'op_'+col+'_nunique')\n",
    "    return op_fea\n",
    "\n",
    "def get_trans_stats_fea(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    \n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "        \n",
    "    tmp = transaction_df.groupby('UID')['day'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns=['UID','trans_day_max','trans_day_min','trans_day_gap']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    tmp = transaction_df.groupby('UID')['trans_amt'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp = pd.merge(tmp, transaction_df.groupby('UID')['trans_amt'].skew().fillna(0), on='UID')\n",
    "    tmp.columns=['UID','trans_amt_max','trans_amt_min','trans_amt_mean', 'trans_amt_gap', 'trans_amt_skew']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    tmp = transaction_df.groupby('UID')['latitude'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_latitude_max', 'trans_latitude_min', 'trans_latitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "    tmp = transaction_df.groupby('UID')['longtitude'].agg([max,min,gap]).reset_index()\n",
    "    tmp.columns = ['UID', 'trans_longtitude_max', 'trans_longtitude_min', 'trans_longtitude_gap']\n",
    "    trans_fea = pd.merge(trans_fea, tmp, on='UID', how='left')\n",
    "\n",
    "    transaction_df['trans_ratio'] = transaction_df['trans_amt'] / transaction_df['bal']\n",
    "    tmp = transaction_df.groupby('UID')['trans_ratio'].agg([max,min,np.mean,gap]).reset_index()\n",
    "    tmp = pd.merge(tmp, transaction_df.groupby('UID')['trans_ratio'].skew().fillna(0), on='UID')\n",
    "    tmp.columns=['UID','trans_ratio_max','trans_ratio_min','trans_ratio_mean', 'trans_ratio_gap', 'trans_ratio_skew']\n",
    "    trans_fea = pd.merge(trans_fea,tmp,on='UID',how='left')\n",
    "    \n",
    "    #trans time\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "    \n",
    "    for col in ['trans_type1','merchant','code1','code2','acc_id1','device_code1','device_code2','device_code3','device1','device2','mac1','ip1','acc_id2','acc_id3','market_code','ip1_sub','geo_code']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "    for col in ['trans_type2','market_type']:\n",
    "        trans_fea = merge_nunique(transaction_df,trans_fea,'UID',col,'trans_'+col+'_nunique')\n",
    "        \n",
    "    return trans_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_counted_features(transaction_df):\n",
    "    trans_counts = transaction_df.groupby('UID').size()\n",
    "    trans_fea = transaction_df[['UID']].drop_duplicates()\n",
    "    #trans_channel\n",
    "    trans_fea = merge_count(transaction_df,trans_fea,'UID','channel','trans_cnt')\n",
    "    trans_fea = merge_nunique(transaction_df,trans_fea,'UID','channel','trans_channel_nunique')\n",
    "    transaction_df['trans_hour'] = transaction_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "#     trans_fea = merge_value_count(transaction_df,trans_fea,['UID','trans_hour'], divisor=trans_counts)\n",
    "    for col in ['market_type']:\n",
    "        trans_fea = merge_value_count(transaction_df,trans_fea,['UID',col], 'trans_%s'%col, divisor=trans_counts)\n",
    "        \n",
    "    trans_fea = merge_value_count(transaction_df,trans_fea,['UID','day_in_week'], 'trans_week_day', divisor=trans_counts)\n",
    "\n",
    "    trans_fea = merge_nunique(transaction_df,trans_fea,'UID','geo_cluster','trans_geo_nunique')\n",
    "    \n",
    "    trans_fea['trans_on_weekend'] = trans_fea[['trans_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    trans_fea['trans_on_weekday'] = trans_fea[['trans_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    pop_count = [\n",
    "        ('device2', 'trans_device2_pop')\n",
    "    ]\n",
    "    for df_name, out_name in pop_count:\n",
    "        pop = getPopularityCount(transaction_df, df_name, out_name)\n",
    "        trans_fea = pd.merge(trans_fea,pop,on='UID',how='left')\n",
    "    \n",
    "    ratio_list = [\n",
    "        ('merchant', 'merchant'),\n",
    "        ('amt_src1', 'amt_src1'),\n",
    "        ('device2', 'trans_device2'),\n",
    "#         ('acc_id2', 'acc_id2'),\n",
    "        ('acc_id3', 'acc_id3'),\n",
    "        ('geo_cluster', 'trans_geo')\n",
    "    ]\n",
    "    \n",
    "    for df_name, out_name in ratio_list:\n",
    "        ratios = get_black_ratio(transaction_df, df_name, out_name)\n",
    "        trans_fea = trans_fea.merge(ratios, on='UID', how='left')\n",
    "    return trans_fea\n",
    "    \n",
    "    \n",
    "def get_op_counted_features(operation_df):\n",
    "    op_counts = operation_df.groupby('UID').size()\n",
    "    \n",
    "    op_fea = operation_df[['UID']].drop_duplicates()\n",
    "#     op_fea = merge_value_count(operation_df,op_fea,['UID','day'],'op_days', divisor=op_counts)\n",
    "    #success count\n",
    "    op_fea = merge_count(operation_df[operation_df.success==0],op_fea,'UID','mode','op_fail_cnt')\n",
    "    op_fea = merge_count(operation_df[operation_df.success==1],op_fea,'UID','mode','op_success_cnt')\n",
    "    op_fea['op_fail_cnt'] = op_fea['op_fail_cnt'].fillna(0)\n",
    "    op_fea['op_success_cnt'] = op_fea['op_success_cnt'].fillna(0)\n",
    "#     op_fea = merge_value_count(operation_df,op_fea,['UID','op_hour'], divisor=op_counts) \n",
    "    op_fea = merge_value_count(operation_df,op_fea,['UID','day_in_week'], 'op_week_day', divisor=op_counts) \n",
    "\n",
    "    op_fea['op_on_weekend'] = op_fea[['op_week_day_%d'%i for i in range(6, 8)]].sum(axis=1)\n",
    "    op_fea['op_on_weekday'] = op_fea[['op_week_day_%d'%i for i in range(1, 6)]].sum(axis=1)\n",
    "    \n",
    "    op_fea = merge_nunique(operation_df,op_fea,'UID','geo_cluster','op_geo_nunique')\n",
    "    \n",
    "    pop_count = [\n",
    "        ('wifi', 'op_wifi_pop'),\n",
    "        ('os', 'op_os_pop'),\n",
    "        ('version', 'op_version_pop'),\n",
    "        ('device2', 'op_device2_pop')\n",
    "    ]\n",
    "    for df_name, out_name in pop_count:\n",
    "        pop = getPopularityCount(operation_df, df_name, out_name)\n",
    "        op_fea = pd.merge(op_fea,pop,on='UID',how='left')\n",
    "    \n",
    "    ratio_list = [\n",
    "        ('device2', 'op_device2'),\n",
    "        ('geo_cluster', 'op_geo')\n",
    "    ]\n",
    "    \n",
    "    for df_name, out_name in ratio_list:\n",
    "        ratios = get_black_ratio(operation_df, df_name, out_name)\n",
    "        op_fea = op_fea.merge(ratios, on='UID', how='left')\n",
    "    \n",
    "    return op_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stats\n",
    "operation_features_stats = get_op_stats_fea(operation_df)\n",
    "transaction_features_stats = get_trans_stats_fea(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_stats = get_op_stats_fea(operation_test)\n",
    "transaction_test_stats = get_trans_stats_fea(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counted data\n",
    "operation_features_counted = get_op_counted_features(operation_df)\n",
    "transaction_features_counted = get_trans_counted_features(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_test_counted = get_op_counted_features(operation_test)\n",
    "transaction_test_counted = get_trans_counted_features(transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = operation_features_stats.merge(transaction_features_stats, on='UID', how='outer')\n",
    "train_features = train_features.merge(operation_features_counted, on='UID', how='outer')\n",
    "train_features = train_features.merge(transaction_features_counted, on='UID', how='outer')\n",
    "train_data = train_features.merge(label,on='UID',how='left')\n",
    "train_data = train_data.drop('UID', axis=1)\n",
    "test_features = operation_test_stats.merge(transaction_test_stats, on='UID', how='outer').sort_values(['UID'])\n",
    "test_features = test_features.merge(operation_test_counted, on='UID', how='outer')\n",
    "test_features = test_features.merge(transaction_test_counted, on='UID', how='outer').sort_values(['UID'])\n",
    "\n",
    "UIDs = test_features['UID']\n",
    "test_features = test_features.drop('UID', axis=1)\n",
    "\n",
    "# columns_to_drop = [col for col in train_data.columns if col not in test_features.columns and col != 'Tag']\n",
    "# train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "# columns_to_drop = [col for col in test_features.columns if col not in train_data.columns and col != 'Tag']\n",
    "# test_features = test_features.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#Fill nan with average\n",
    "train_data = train_data.fillna(train_data.mean())\n",
    "test_features = test_features.fillna(train_data.drop('Tag', axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "def preprocess(x):\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_scorer(estimator, x, y_true):\n",
    "    y_predict = estimator.predict_proba(x)[:, 1]\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    \n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import lightgbm as lgb\n",
    "import sklearn.ensemble\n",
    "import xgboost as xgb\n",
    "\n",
    "def cv(x, y, params={}, splits=5):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    kfold = sklearn.model_selection.StratifiedKFold(splits, shuffle=True)\n",
    "    cv_score = sklearn.model_selection.cross_validate(clf, x, y, cv=kfold, scoring={\n",
    "        'tpr': tpr_scorer,\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_micro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }, return_train_score=True)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        \"num_leaves\": 200,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        'min_child_samples': 100,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "        'boost_from_average': True,\n",
    "        'min_child_weight': 1e-3,\n",
    "        \"subsample_for_bin\": 20000,\n",
    "        'max_bin': 512,\n",
    "        \"metric\": 'auc',\n",
    "        'reg_alpha': 3,\n",
    "        'reg_lambda': 5,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree':0.7, \n",
    "        'subsample_freq': 1,\n",
    "        'n_jobs': -1,\n",
    "}\n",
    "# params = {\n",
    "#         'booster':'gbtree',\n",
    "#         'objective':'binary:logistic',\n",
    "#         'stratified':True,\n",
    "#         'max_depth':8,\n",
    "#         'min_child_weight':1,\n",
    "#         'gamma':3,\n",
    "#         'subsample':0.8,#0.7\n",
    "#         'colsample_bytree':0.6, \n",
    "#         'reg_lambda':3, \n",
    "#         'eta':0.05,\n",
    "#         'seed':20,\n",
    "#         'silent':1,\n",
    "#         'eval_metric':'auc',\n",
    "#         'n_jobs': 12,\n",
    "#         'n_estimators': 400,\n",
    "#         'early_stopping_round': 200,\n",
    "#         'scale_pos_weight': float(label[label.Tag==0].shape[0]) / label[label.Tag==1].shape[0],\n",
    "#         'verbosity': 2\n",
    "# }\n",
    "\n",
    "def run_cross_validation(x, y):\n",
    "    cv_result = cv(x, y, params=params, splits=5)\n",
    "    for scorer, score in cv_result.items():\n",
    "        print('%s: %s' % (scorer, score))\n",
    "        print('Average %s: %f' % (scorer, score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: [10.01028705  8.00668812  8.0604279  11.73915696 12.58610582]\n",
      "Average fit_time: 10.080533\n",
      "score_time: [0.41855907 0.40110207 0.42136788 0.71816707 0.50397205]\n",
      "Average score_time: 0.492634\n",
      "test_tpr: [0.84632439 0.83932322 0.83500583 0.85810968 0.84807468]\n",
      "Average test_tpr: 0.845368\n",
      "train_tpr: [1. 1. 1. 1. 1.]\n",
      "Average train_tpr: 1.000000\n",
      "test_accuracy: [0.97370109 0.9712957  0.97434253 0.97690827 0.97385726]\n",
      "Average test_accuracy: 0.974021\n",
      "train_accuracy: [0.99979954 0.99955899 0.99975945 0.99979954 0.99971937]\n",
      "Average train_accuracy: 0.999727\n",
      "test_f1: [0.97370109 0.9712957  0.97434253 0.97690827 0.97385726]\n",
      "Average test_f1: 0.974021\n",
      "train_f1: [0.99979954 0.99955899 0.99975945 0.99979954 0.99971937]\n",
      "Average train_f1: 0.999727\n",
      "test_roc_auc: [0.99212765 0.99051782 0.99014557 0.99206528 0.99121144]\n",
      "Average test_roc_auc: 0.991214\n",
      "train_roc_auc: [0.99999997 0.99999995 0.99999998 0.99999999 0.99999995]\n",
      "Average train_roc_auc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation(train_data.drop('Tag', axis=1).fillna(train_data.drop('Tag', axis=1).mean()).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, params={}):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(train_data.drop('Tag', axis=1).values, train_data.loc[:, 'Tag'].values, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_proba(test_features.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09680107699211488"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_scorer(model, train_data.drop('Tag', axis=1).fillna(train_data.drop('Tag', axis=1).mean()).values, train_data.loc[:, 'Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame()\n",
    "result_frame['UID'] = UIDs\n",
    "result_frame['Tag'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_frame.to_csv(res_path + 'res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('op_day_max', 717),\n",
       " ('op_day_min', 790),\n",
       " ('op_day_gap', 516),\n",
       " ('op_cnt', 1523),\n",
       " ('op_mode_nunique', 988),\n",
       " ('op_os_nunique', 38),\n",
       " ('op_version_nunique', 26),\n",
       " ('op_device1_nunique', 39),\n",
       " ('op_device2_nunique', 17),\n",
       " ('op_device_code1_nunique', 37),\n",
       " ('op_device_code2_nunique', 148),\n",
       " ('op_mac1_nunique', 67),\n",
       " ('op_ip1_nunique', 488),\n",
       " ('op_ip2_nunique', 96),\n",
       " ('op_device_code3_nunique', 62),\n",
       " ('op_mac2_nunique', 109),\n",
       " ('op_wifi_nunique', 195),\n",
       " ('op_ip1_sub_nunique', 543),\n",
       " ('op_ip2_sub_nunique', 45),\n",
       " ('op_geo_code_nunique', 123),\n",
       " ('trans_day_max', 643),\n",
       " ('trans_day_min', 660),\n",
       " ('trans_day_gap', 231),\n",
       " ('trans_amt_max', 441),\n",
       " ('trans_amt_min', 664),\n",
       " ('trans_amt_mean', 962),\n",
       " ('trans_amt_gap', 640),\n",
       " ('trans_amt_skew', 940),\n",
       " ('trans_ratio_max', 297),\n",
       " ('trans_ratio_min', 947),\n",
       " ('trans_ratio_mean', 803),\n",
       " ('trans_ratio_gap', 474),\n",
       " ('trans_ratio_skew', 1184),\n",
       " ('trans_trans_type1_nunique', 52),\n",
       " ('trans_merchant_nunique', 215),\n",
       " ('trans_code1_nunique', 41),\n",
       " ('trans_code2_nunique', 0),\n",
       " ('trans_acc_id1_nunique', 142),\n",
       " ('trans_device_code1_nunique', 42),\n",
       " ('trans_device_code2_nunique', 81),\n",
       " ('trans_device_code3_nunique', 33),\n",
       " ('trans_device1_nunique', 20),\n",
       " ('trans_device2_nunique', 18),\n",
       " ('trans_mac1_nunique', 19),\n",
       " ('trans_ip1_nunique', 223),\n",
       " ('trans_acc_id2_nunique', 18),\n",
       " ('trans_acc_id3_nunique', 6),\n",
       " ('trans_market_code_nunique', 103),\n",
       " ('trans_ip1_sub_nunique', 318),\n",
       " ('trans_geo_code_nunique', 64),\n",
       " ('trans_trans_type2_nunique', 77),\n",
       " ('trans_market_type_nunique', 17),\n",
       " ('op_fail_cnt', 511),\n",
       " ('op_success_cnt', 1471),\n",
       " ('op_week_day_1', 620),\n",
       " ('op_week_day_2', 497),\n",
       " ('op_week_day_3', 599),\n",
       " ('op_week_day_4', 701),\n",
       " ('op_week_day_5', 599),\n",
       " ('op_week_day_6', 622),\n",
       " ('op_week_day_7', 637),\n",
       " ('op_on_weekend', 682),\n",
       " ('op_on_weekday', 339),\n",
       " ('op_geo_nunique', 54),\n",
       " ('op_wifi_pop_max', 724),\n",
       " ('op_wifi_pop_min', 428),\n",
       " ('op_wifi_pop_avg', 900),\n",
       " ('op_wifi_pop_skew', 489),\n",
       " ('op_os_pop_max', 8),\n",
       " ('op_os_pop_min', 122),\n",
       " ('op_os_pop_avg', 729),\n",
       " ('op_os_pop_skew', 1089),\n",
       " ('op_version_pop_max', 130),\n",
       " ('op_version_pop_min', 115),\n",
       " ('op_version_pop_avg', 1130),\n",
       " ('op_version_pop_skew', 1292),\n",
       " ('op_device2_pop_max', 846),\n",
       " ('op_device2_pop_min', 847),\n",
       " ('op_device2_pop_avg', 910),\n",
       " ('op_device2_pop_skew', 286),\n",
       " ('max_op_device2_black_ratio', 451),\n",
       " ('min_op_device2_black_ratio', 1524),\n",
       " ('avg_op_device2_black_ratio', 2146),\n",
       " ('skew_op_device2_black_ratio', 2592),\n",
       " ('max_op_geo_black_ratio', 387),\n",
       " ('min_op_geo_black_ratio', 821),\n",
       " ('avg_op_geo_black_ratio', 1606),\n",
       " ('skew_op_geo_black_ratio', 1804),\n",
       " ('trans_cnt', 715),\n",
       " ('trans_channel_nunique', 59),\n",
       " ('trans_market_type_1.0', 16),\n",
       " ('trans_market_type_2.0', 94),\n",
       " ('trans_week_day_1', 93),\n",
       " ('trans_week_day_2', 52),\n",
       " ('trans_week_day_3', 54),\n",
       " ('trans_week_day_4', 108),\n",
       " ('trans_week_day_5', 47),\n",
       " ('trans_week_day_6', 101),\n",
       " ('trans_week_day_7', 94),\n",
       " ('trans_geo_nunique', 11),\n",
       " ('trans_on_weekend', 100),\n",
       " ('trans_on_weekday', 32),\n",
       " ('trans_device2_pop_max', 804),\n",
       " ('trans_device2_pop_min', 797),\n",
       " ('trans_device2_pop_avg', 606),\n",
       " ('trans_device2_pop_skew', 104),\n",
       " ('max_merchant_black_ratio', 1913),\n",
       " ('min_merchant_black_ratio', 2015),\n",
       " ('avg_merchant_black_ratio', 1846),\n",
       " ('skew_merchant_black_ratio', 608),\n",
       " ('max_amt_src1_black_ratio', 390),\n",
       " ('min_amt_src1_black_ratio', 259),\n",
       " ('avg_amt_src1_black_ratio', 1580),\n",
       " ('skew_amt_src1_black_ratio', 1429),\n",
       " ('max_trans_device2_black_ratio', 1185),\n",
       " ('min_trans_device2_black_ratio', 1343),\n",
       " ('avg_trans_device2_black_ratio', 977),\n",
       " ('skew_trans_device2_black_ratio', 144),\n",
       " ('max_acc_id3_black_ratio', 51),\n",
       " ('min_acc_id3_black_ratio', 52),\n",
       " ('avg_acc_id3_black_ratio', 161),\n",
       " ('skew_acc_id3_black_ratio', 16),\n",
       " ('max_trans_geo_black_ratio', 543),\n",
       " ('min_trans_geo_black_ratio', 563),\n",
       " ('avg_trans_geo_black_ratio', 704),\n",
       " ('skew_trans_geo_black_ratio', 265)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(train_data.drop('Tag', axis=1).columns[i], model.feature_importances_[i]) for i in range(len(model.feature_importances_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
